{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and dataset\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Adjust pandas display options\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.expand_frame_repr', False)  # Prevent truncation\n",
    "\n",
    "\n",
    "df = pd.read_csv('/Users/joezhou/Downloads/Mentum Assignment Data/Customer-churn-records.csv',sep=\",\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2 \n",
    "# Now that you have access to the dataset, it’s time to start understanding the statistical properties of the data. These activities will all be carried out in Ed Lessons.\n",
    "# Calculate basic statistical measures such as mean, median, mode, and percentiles for each attribute. \n",
    "# Recognise common distributions in the data, such as Gaussian, binomial, Poisson, etc. \n",
    "# Plan what visualisations you will include in your report and describe how you will explore and visualise the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2 \n",
    "# Calculate basic statistical measures such as mean, median, mode, and percentiles for each attribute. \n",
    "\n",
    "# identify variable types and records\n",
    "df.info()\n",
    "\n",
    "# basic statistics\n",
    "df.describe().round(1).applymap(lambda x: f'{x:,.1f}').transpose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2 \n",
    "\n",
    "# Identify mode for every variable and respective frequencies\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "mode_freq = {}\n",
    "\n",
    "for column in df.columns:\n",
    "    # Calculate the mode(s) of the column\n",
    "    mode_values = df[column].mode()\n",
    "    \n",
    "    # Create an empty list to store mode and frequency pairs\n",
    "    mode_list = []\n",
    "    \n",
    "    for mode in mode_values:\n",
    "        # Calculate the frequency of the mode\n",
    "        frequency = df[column].value_counts().get(mode, 0)\n",
    "        mode_list.append((mode, frequency))\n",
    "    \n",
    "    # Store the mode and its frequency in the dictionary\n",
    "    mode_freq[column] = mode_list\n",
    "\n",
    "# Print the results\n",
    "for column, modes in mode_freq.items():\n",
    "    print(f'Mode(s) of {column}:')\n",
    "    for mode, frequency in modes:\n",
    "        print(f'  Value: {mode}, Frequency: {frequency}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2 \n",
    "\n",
    "# distribution graphs\n",
    "# List of numerical and categorical columns, as some are numerical whilest others are categorical\n",
    "numerical_columns = ['CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts',\n",
    "                     'EstimatedSalary', 'Satisfaction Score', 'Point Earned']\n",
    "\n",
    "# identified as objects in the underlying data\n",
    "categorical_columns = ['Geography', 'Gender', 'HasCrCard', 'IsActiveMember', 'Exited', 'Complain', 'Card Type']\n",
    "\n",
    "# Combine the lists to get the total number of variables\n",
    "all_columns = numerical_columns + categorical_columns\n",
    "total_columns = len(all_columns)\n",
    "\n",
    "# Determine grid size (e.g., 3 columns)\n",
    "n_cols = 3\n",
    "n_rows = (total_columns // n_cols) + (total_columns % n_cols > 0)\n",
    "\n",
    "# Set up the plotting style and figure size\n",
    "sns.set(style=\"whitegrid\")\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, n_rows * 5))\n",
    "\n",
    "# Flatten the axes array for easy indexing\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Create plots for each variable\n",
    "for i, col in enumerate(all_columns):\n",
    "    if col in numerical_columns:\n",
    "        sns.histplot(df[col], kde=True, bins=30, ax=axes[i])\n",
    "    else:\n",
    "        sns.countplot(x=col, data=df, ax=axes[i])\n",
    "    \n",
    "    axes[i].set_title(f'Distribution of {col}')\n",
    "    axes[i].set_xlabel(col)\n",
    "    axes[i].set_ylabel('Frequency' if col in numerical_columns else 'Count')\n",
    "\n",
    "# Remove any unused subplots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 3\n",
    "# Generating relevant charts to visualise the distributions of different attributes will be your starting point for this part. \n",
    "# Generate histograms, box plots and other relevant charts to visualise data distributions\n",
    "# Generate a correlation matrix to understand the relationships between different attributes. \n",
    "# Identify and address any data quality issues in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 3 \n",
    "\n",
    "# List of numerical and categorical columns\n",
    "numerical_columns = ['CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts',\n",
    "                     'EstimatedSalary', 'Satisfaction Score', 'Point Earned']\n",
    "\n",
    "categorical_columns = ['Geography', 'Gender', 'HasCrCard', 'IsActiveMember', \n",
    "                       'Complain', 'Card Type']\n",
    "\n",
    "# Combine the lists to get the total number of variables\n",
    "all_columns = numerical_columns + categorical_columns\n",
    "total_columns = len(all_columns)\n",
    "\n",
    "# Determine grid size (e.g., 4 columns)\n",
    "n_cols = 4\n",
    "n_rows = (total_columns // n_cols) + (total_columns % n_cols > 0)\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, n_rows * 5))\n",
    "\n",
    "# Flatten the axes array for easy indexing\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Create plots for each variable\n",
    "for i, col in enumerate(all_columns):\n",
    "    if col in numerical_columns:\n",
    "        sns.boxplot(x='Exited', y=col, data=df, ax=axes[i])\n",
    "        axes[i].set_title(f'Box Plot of {col}')\n",
    "        axes[i].set_xlabel('Exited')\n",
    "        axes[i].set_ylabel(col)\n",
    "    else:\n",
    "        sns.countplot(x=col, hue='Exited', data=df, ax=axes[i])\n",
    "        axes[i].set_title(f'Count Plot of {col}')\n",
    "        axes[i].set_xlabel(col)\n",
    "        axes[i].set_ylabel('Count')\n",
    "\n",
    "# Remove any unused subplots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 3\n",
    "\n",
    "# Convert categorical variables into numerical variables to understand relationship to churn for correlation analysis\n",
    "\n",
    "df_encoded = pd.get_dummies(df, columns=['Geography', 'Gender', 'HasCrCard', 'IsActiveMember', 'Complain', 'Card Type'], drop_first=False)\n",
    "\n",
    "# Identify and convert boolean columns to integers\n",
    "bool_columns = df_encoded.select_dtypes(include='bool').columns\n",
    "\n",
    "df_encoded[bool_columns] = df_encoded[bool_columns].astype(int)\n",
    "\n",
    "# df_encoded.head()\n",
    "# df_encoded.info()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 3: create heatmap and correlation chart \n",
    "\n",
    "# List of shortlisted columns\n",
    "shortlist_columns = [#outcome variable used for prediction\n",
    "                     'Exited',\n",
    "                     #Numerical variables from source file\n",
    "                     'CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts',\n",
    "                     'EstimatedSalary', 'Satisfaction Score','Point Earned',\n",
    "                    #transformed variable from categories\n",
    "                    'Geography_NSW', 'Geography_QLD', 'Geography_VIC','Gender_Female','Gender_Male',\n",
    "                    'HasCrCard_0','HasCrCard_1','IsActiveMember_0','IsActiveMember_1','Complain_0','Complain_1',\n",
    "                    'Card Type_DIAMOND', 'Card Type_GOLD','Card Type_PLATINUM','Card Type_SILVER'\n",
    "                     ]\n",
    "\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "correlation_matrix = df_encoded[shortlist_columns].corr()\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "# Draw the heatmap\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\",annot_kws={\"size\": 8}, cmap='coolwarm',\n",
    "            cbar_kws={'label': 'Correlation Coefficient'}, linewidths=2)\n",
    "\n",
    "# Set title and show the plot\n",
    "plt.title('Correlation Heatmap of Numerical Variables')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Find pairs with correlation coefficient >= 0.8\n",
    "high_corr_pairs = []\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if abs(correlation_matrix.iloc[i, j]) >= 0.8:\n",
    "            high_corr_pairs.append((correlation_matrix.columns[i], correlation_matrix.columns[j], correlation_matrix.iloc[i, j]))\n",
    "\n",
    "# Display the pairs and their R-squared values\n",
    "for pair in high_corr_pairs:\n",
    "    var1, var2, corr = pair\n",
    "    r_squared = corr ** 2\n",
    "    print(f\"Variables: {var1} and {var2} have a correlation of {corr:.2f} and R-squared of {r_squared:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data quality checks, for part 3\n",
    "# identify, missing values, duplicates, data types, value range and unique values in categorical columns\n",
    "\n",
    "\n",
    "# Check for duplicate rows\n",
    "print(df[df.duplicated(subset=df.columns)])\n",
    " \n",
    "# Check for duplicate primary keys\n",
    "print(df[df.duplicated(subset='CustomerId')])\n",
    "\n",
    "# note the function below common practice used within professional context\n",
    "\n",
    "def check_data_quality(df):\n",
    "\n",
    "    # Check for missing values\n",
    "    print(\"Missing Values:\")\n",
    "    missing_values = df.isnull().sum()\n",
    "    missing_values = missing_values[missing_values > 0]\n",
    "    if not missing_values.empty:\n",
    "        print(missing_values)\n",
    "    else:\n",
    "        print(\"No missing values found.\")\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # Check for duplicates\n",
    "    print(\"Duplicate Rows:\")\n",
    "    duplicates = df.duplicated().sum()\n",
    "    if duplicates > 0:\n",
    "        print(f\"Number of duplicate rows: {duplicates}\")\n",
    "    else:\n",
    "        print(\"No duplicate rows found.\")\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # Check for incorrect data types\n",
    "    print(\"Data Types:\")\n",
    "    incorrect_types = df.dtypes[df.dtypes == 'object'].index\n",
    "    print(f\"Object types: {list(incorrect_types)}\")\n",
    "    \n",
    "    # Check for value ranges and outliers (for numerical columns)\n",
    "    print(\"\\nValue Ranges and Outliers:\")\n",
    "    for column in df.select_dtypes(include=['int64', 'float64']).columns:\n",
    "        min_val = df[column].min()\n",
    "        max_val = df[column].max()\n",
    "        print(f\"{column}: Min = {min_val}, Max = {max_val}\")\n",
    "        \n",
    "        # Outliers can be detected using z-scores or IQR, here we use IQR\n",
    "        Q1 = df[column].quantile(0.25)\n",
    "        Q3 = df[column].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        outliers = df[(df[column] < (Q1 - 1.5 * IQR)) | (df[column] > (Q3 + 1.5 * IQR))]\n",
    "        if not outliers.empty:\n",
    "            print(f\"Outliers detected in {column}: {len(outliers)} rows\")\n",
    "        else:\n",
    "            print(f\"No outliers detected in {column}\")\n",
    "\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # Check for unique values in categorical columns\n",
    "    print(\"Unique Values in Categorical Columns:\")\n",
    "    for column in df.select_dtypes(include=['object']).columns:\n",
    "        unique_values = df[column].unique()\n",
    "        print(f\"{column}: {len(unique_values)} unique values\")\n",
    "\n",
    "# Call function\n",
    "check_data_quality(df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 3: sample data and bootstrapping\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#==================================================================================\n",
    "# Part 3.1 determining sample size\n",
    "\n",
    "# Given parameters\n",
    "confidence_level = 0.85\n",
    "margin_of_error = 0.25\n",
    "population_size = 10000\n",
    "\n",
    "# Z-score for 85% confidence level\n",
    "Z = stats.norm.ppf(0.5 + confidence_level / 2)\n",
    "\n",
    "# Assuming p = 0.5 for maximum variability\n",
    "p = 0.5\n",
    "\n",
    "# Calculating the required sample size without finite population correction\n",
    "sample_size = (Z**2 * p * (1 - p)) / (margin_of_error**2)\n",
    "\n",
    "# Apply finite population correction\n",
    "sample_size = sample_size / (1 + (sample_size - 1) / population_size)\n",
    "sample_size = int(np.ceil(sample_size))  # Round up to the nearest integer\n",
    "print(f\"Required sample size with population correction: {sample_size}\")\n",
    "#==================================================================================\n",
    "# Part 3.2 Bootstrapping\n",
    "\n",
    "num_bootstrap_samples = 9\n",
    "bootstrap_samples = np.random.choice(df.index, size=(num_bootstrap_samples, sample_size), replace=True)\n",
    "\n",
    "# List to store results from bootstrapping\n",
    "boot_means = []\n",
    "\n",
    "# Apply bootstrapping\n",
    "for sample_indices in bootstrap_samples:\n",
    "    sample_df = df.loc[sample_indices]\n",
    "    boot_means.append(sample_df['Exited'].mean())\n",
    "    bootstrap_std_of_means = np.std(boot_means)\n",
    "\n",
    "# Calculate the population mean for comparison\n",
    "population_mean = df['Exited'].mean()\n",
    "population_std = df['Exited'].std()\n",
    "\n",
    "# Display results\n",
    "boot_mean = np.mean(boot_means)\n",
    "print(f\"Bootstrapped mean: {boot_mean},Bootstrap Standard Deviation of Means: {bootstrap_std_of_means:.2f}\")\n",
    "print(f\"Population mean: {population_mean},Population Standard Deviation: {population_std:.2f}\")\n",
    "\n",
    "# The bootstrapped mean closely approximates the original mean, suggesting the sample means are centered around the true population mean. \n",
    "# However, the bootstrapped standard deviation is considerably smaller than the original standard deviation, \n",
    "# indicating that the sample means exhibit less variability than individual data points in the original dataset. \n",
    "\n",
    "#==================================================================================\n",
    "# Part 3.3 Calculate confidence interval for bootstrapped means\n",
    "conf_interval = np.percentile(boot_means, [(1-confidence_level)/2*100, (1+confidence_level)/2*100])\n",
    "print(f\"{int(confidence_level*100)}% Confidence interval for 'Exited': {conf_interval}\")\n",
    "\n",
    "#==================================================================================\n",
    "# Part 3.4 Generate charts for presentation\n",
    "\n",
    "# 1. Histogram of Bootstrapped Means vs. Population Mean\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.hist(boot_means, bins=30, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "plt.axvline(population_mean, color='red', linestyle='dashed', linewidth=2)\n",
    "plt.title('Bootstrapped Means vs. Population Mean')\n",
    "plt.xlabel('Mean of Exited')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend(['Population Mean', 'Bootstrapped Means'])\n",
    "plt.show()\n",
    "\n",
    "# 2. Histogram of Bootstrapped Standard Deviations vs. Population Standard Deviation\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.hist(bootstrap_std_of_means, bins=30, color='lightgreen', edgecolor='black', alpha=0.7)\n",
    "plt.axvline(population_std, color='orange', linestyle='dashed', linewidth=2)\n",
    "plt.title('Bootstrapped Standard Deviations vs. Population Standard Deviation')\n",
    "plt.xlabel('Standard Deviation of Exited')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend(['Population Std Dev', 'Bootstrapped Std Devs'])\n",
    "plt.show()\n",
    "\n",
    "# 3. Confidence Interval Visualization\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.hist(boot_means, bins=30, color='lightcoral', edgecolor='black', alpha=0.7)\n",
    "plt.axvline(conf_interval[0], color='blue', linestyle='dashed', linewidth=2)\n",
    "plt.axvline(conf_interval[1], color='blue', linestyle='dashed', linewidth=2)\n",
    "plt.axvline(population_mean, color='red', linestyle='dashed', linewidth=2)\n",
    "plt.title(f'{int(confidence_level*100)}% Confidence Interval for Bootstrapped Means')\n",
    "plt.xlabel('Mean of Exited')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend(['Lower CI', 'Upper CI', 'Population Mean'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 4\n",
    "# For this final part, formulate hypotheses related to customer churn. For example, ‘customers with a lower balance are more likely to churn.’ \n",
    "# Apply statistical significance tests to evaluate these hypotheses, interpret the results of your statistical tests and draw valid conclusions. This will help you gain an understanding of the principles of experimental design.\n",
    "# Create a plan for a controlled experiment to test one of your hypotheses. Include the experiment design in your PowerPoint report.\n",
    "# Write a conclusion for your report and include it in the PowerPoint.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part 4\n",
    "# Hypothesis testing\n",
    "\n",
    "# variable = age\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Split the data into groups\n",
    "exited = df[df['Exited'] == 1]['Age']\n",
    "not_exited = df[df['Exited'] == 0]['Age']\n",
    "\n",
    "# Perform t-test\n",
    "t_stat, p_value = stats.ttest_ind(exited, not_exited, equal_var=False)\n",
    "print(\"Age:\",f\"t-statistic: {t_stat.round(2)}, p-value: {p_value.round(2)}\")\n",
    "\n",
    "\n",
    "# variable = Tenure\n",
    "# Split the data into groups\n",
    "exited = df[df['Exited'] == 1]['Tenure']\n",
    "not_exited = df[df['Exited'] == 0]['Tenure']\n",
    "\n",
    "# Perform t-test\n",
    "t_stat, p_value = stats.ttest_ind(exited, not_exited, equal_var=False)\n",
    "print(\"Tenure:\",f\"t-statistic: {t_stat.round(2)}, p-value: {p_value.round(2)}\")\n",
    "\n",
    "# Variable = Geography\n",
    "# Create a contingency table\n",
    "contingency_table = pd.crosstab(df['Geography'], df['Exited'])\n",
    "\n",
    "# Perform Chi-Square test\n",
    "chi2, p, dof, ex = stats.chi2_contingency(contingency_table)\n",
    "print(\"Geography:\",f\"Chi2: {chi2.round(2)}, p-value: {p.round(2)}\")\n",
    "\n",
    "\n",
    "# Variable = Complain\n",
    "# Create a contingency table\n",
    "contingency_table = pd.crosstab(df['Complain'], df['Exited'])\n",
    "\n",
    "# Perform Chi-Square test\n",
    "chi2, p, dof, ex = stats.chi2_contingency(contingency_table)\n",
    "print(\"Complain:\",f\"Chi2: {chi2.round(2)}, p-value: {p.round(2)}\")\n",
    "\n",
    "# Variable = Satisfaction Score\n",
    "# Split the data into groups\n",
    "exited = df[df['Exited'] == 1]['Satisfaction Score']\n",
    "not_exited = df[df['Exited'] == 0]['Satisfaction Score']\n",
    "\n",
    "# Perform t-test\n",
    "t_stat, p_value = stats.ttest_ind(exited, not_exited, equal_var=False)\n",
    "print(\"Satisfaction Score:\",f\"t-statistic: {t_stat.round(2)}, p-value: {p_value.round(2)}\")\n",
    "\n",
    "# Variable = Credit Score\n",
    "# Split the data into groups\n",
    "exited = df[df['Exited'] == 1]['CreditScore']\n",
    "not_exited = df[df['Exited'] == 0]['CreditScore']\n",
    "\n",
    "# Perform t-test\n",
    "t_stat, p_value = stats.ttest_ind(exited, not_exited, equal_var=False)\n",
    "print(\"Credit Score:\",f\"t-statistic: {t_stat.round(2)}, p-value: {p_value.round(2)}\")\n",
    "\n",
    "# Varible = Balance\n",
    "# Split the data into groups\n",
    "exited = df[df['Exited'] == 1]['Balance']\n",
    "not_exited = df[df['Exited'] == 0]['Balance']\n",
    "\n",
    "# Perform t-test\n",
    "t_stat, p_value = stats.ttest_ind(exited, not_exited, equal_var=False)\n",
    "print(\"Balance:\",f\"t-statistic: {t_stat.round(2)}, p-value: {p_value.round(2)}\")\n",
    "\n",
    "# Variale = active member\n",
    "# Create a contingency table\n",
    "contingency_table = pd.crosstab(df['IsActiveMember'], df['Exited'])\n",
    "\n",
    "# Perform Chi-Square test\n",
    "chi2, p, dof, ex = stats.chi2_contingency(contingency_table)\n",
    "print(\"Active Member:\",f\"Chi2: {chi2.round(2)}, p-value: {p.round(2)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
